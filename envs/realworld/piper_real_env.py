"""
Piper Real Environment for Policy Deployment
æä¾›ä¸ Base_Task å…¼å®¹çš„æ¥å£ï¼Œç”¨äºçœŸæœºéƒ¨ç½²
"""
import time
import numpy as np
from typing import Any, Dict, Optional, List
from pathlib import Path
import cv2
import open3d as o3d
from termcolor import cprint

from .piper import PiperRobot
from .camera import RealSenseCamera, get_device_ids
import pyrealsense2 as rs


import torch
import sys
sys.path.append('/home/dex/haoran/gello_software/third_party/pointnet2')
import pointnet2_utils


class PiperRealEnv:
    """Piper çœŸæœºç¯å¢ƒç±»
    
    æä¾›ä¸ä»¿çœŸç¯å¢ƒ Base_Task ç›¸ä¼¼çš„æ¥å£ï¼Œä½¿å¾—æ‰€æœ‰ policy å¯ä»¥æ— ç¼è¿ç§»åˆ°çœŸæœº
    """
    
    def __init__(
        self,
        policy: str = "unknown_policy",
        robot_ip: str = "can_right",
        init_pos = [0, 0, 0, 0, 0, 0, 0],
        step_lim: int = 1000,
        img_size: tuple = (640, 480),
    ):
        """
        Args:
            camera_config: ç›¸æœºé…ç½®å­—å…¸ï¼ŒåŒ…å«å„ä¸ªç›¸æœºçš„ device_id
            robot_ip: æœºå™¨äºº CAN æ¥å£åç§°
            step_lim: æœ€å¤§æ‰§è¡Œæ­¥æ•°
            img_size: å›¾åƒå°ºå¯¸ (width, height)
        """
        cprint("=" * 50, "cyan")
        cprint(f"åˆå§‹åŒ– Piper {policy} çœŸæœºç¯å¢ƒ...", "cyan", attrs=["bold"])
        cprint("=" * 50, "cyan")

        self.policy = policy
        
        # åˆå§‹åŒ–æœºå™¨äºº
        cprint(f"ğŸ¤– è¿æ¥æœºå™¨äºº: {robot_ip}", "yellow")
        self.robot = PiperRobot(robot_ip=robot_ip)

        self.init_pos = np.array(init_pos)
        
        print("åˆå§‹åŒ–ç›¸æœº...")
        # d455
        # self.camera = RealSenseCamera(device_id='215122251612', flip=False)
        # self.intrinsics = rs.intrinsics()
        # self.intrinsics.width, self.intrinsics.height = 640, 480
        # self.intrinsics.ppx, self.intrinsics.ppy = 323.6994934082031, 240.37545776367188
        # self.intrinsics.fx, self.intrinsics.fy = 382.5924072265625, 382.1819763183594
        # self.intrinsics.model = rs.distortion.brown_conrady
        # self.intrinsics.coeffs = [-0.05781254917383194, 0.07238195091485977, 0.00010194736387347803,
        #                     0.0006292760954238474, -0.023512376472353935]

        self.camera = RealSenseCamera(device_id='f1271156', flip=False)
        # L515-depth
        # self.intrinsics = rs.intrinsics()
        # self.intrinsics.width, self.intrinsics.height = 640, 480
        # self.intrinsics.ppx, self.intrinsics.ppy = 301.09375, 246.337890625
        # self.intrinsics.fx, self.intrinsics.fy = 459.8203125, 459.96484375
        # self.intrinsics.model = rs.distortion.none
        # self.intrinsics.coeffs = [0.0, 0.0, 0.0, 0.0, 0.0]

        # L515-rgb
        self.intrinsics = rs.intrinsics()
        self.intrinsics.width, self.intrinsics.height = 640, 480
        self.intrinsics.ppx, self.intrinsics.ppy = 330.53131103515625, 232.83041381835938
        self.intrinsics.fx, self.intrinsics.fy = 598.9841918945312, 599.3632202148438
        self.intrinsics.model = rs.distortion.brown_conrady
        self.intrinsics.coeffs = [0.16919225454330444, -0.5201395750045776, -0.0035975882783532143, -0.00044879087363369763, 0.4867783486843109]

        # GPUæ—‹è½¬
        theta_x = torch.deg2rad(torch.tensor(140., device='cuda'))
        theta_z = torch.deg2rad(torch.tensor(2.5, device='cuda'))
        R_x = torch.tensor([
            [1, 0, 0],
            [0, torch.cos(theta_x), -torch.sin(theta_x)],
            [0, torch.sin(theta_x), torch.cos(theta_x)]
        ], device='cuda')
        R_z = torch.tensor([
            [torch.cos(theta_z), -torch.sin(theta_z), 0],
            [torch.sin(theta_z), torch.cos(theta_z), 0],
            [0, 0, 1]
        ], device='cuda')
        R = R_z @ R_x
        self.R = R.T

        
        self.img_size = img_size

        self.instruction = None
        self.instruction_sim = None
        self.instruction_int = None
        
        # ç¯å¢ƒçŠ¶æ€
        self.step_lim = step_lim
        self.take_action_cnt = 0
        self.eval_success = False
        self.suc = 0
        self.test_num = 0

        self.first_time = True
        self.reset()
        
        cprint("âœ… Piper çœŸæœºç¯å¢ƒåˆå§‹åŒ–å®Œæˆ!", "green", attrs=["bold"])
        cprint("=" * 50, "cyan")
    
    def dofs(self) -> int:
        return 7
    
    def _load_data_to_memory(self):
        """å°†HDF5æ•°æ®é¢„åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¹¶è§£ç RGBå›¾åƒ"""

        self.data_path = "/home/dex/haoran/gello_software/data_processed_10hz/test_bbhlr_enhance/hands_10hz/data/episode0.hdf5"


        import h5py
        with h5py.File(self.data_path, 'r') as h5_file:
            # è·å–æ•°æ®é•¿åº¦
            self.data_length = len(h5_file['joint_action']['vector'])
            
            # é¢„åŠ è½½å…³èŠ‚çŠ¶æ€æ•°æ®
            self.joint_state_array = np.array(h5_file['joint_state']['vector'][:])

            self.endpose_array = np.array(h5_file['endpose'][:])
            
            # é¢„åŠ è½½å¹¶è§£ç RGBå›¾åƒæ•°æ®
            # é¢„åŠ è½½ç‚¹äº‘æ•°æ®
            self.pointcloud_array = np.array(h5_file['pointcloud'][:])
            
            # é¢„åŠ è½½åŠ¨ä½œæ•°æ®ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            self.joint_action_array = np.array(h5_file['joint_state']['vector'][:])
            
        cprint("\næ•°æ®ç»“æ„:", "yellow")
        cprint(f"  - joint_state_array: {self.joint_state_array.shape}", "white")
        cprint(f"  - pointcloud_array: {self.pointcloud_array.shape}", "white")
        cprint(f"  - joint_action_array: {self.joint_action_array.shape}", "white")
        print()

    def get_obs_dataset(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ­¥çš„è§‚æµ‹æ•°æ®ï¼ˆä»å†…å­˜ä¸­è¯»å–ï¼‰
        
        Returns:
            observation: åŒ…å« point_cloud å’Œ agent_pos çš„å­—å…¸
        """
        step = min(self.take_action_cnt, self.data_length - 1)
        
        cprint(f"ğŸ“¸ ä»æ•°æ®é›†ä¸­è·å–è§‚æµ‹: step {step}/{self.data_length}", "cyan")
        

        # DP3 éœ€è¦ joint_action.vector å’Œ pointcloud
        obs = {
            "joint_action": {},
            "endpose": None,
            "pointcloud": None,
            "instruction": None,
            "instruction_sim": None,
            "instruction_int": None
        }
        
        # 1. è¯»å–å…³èŠ‚çŠ¶æ€
        joint_vector = self.joint_state_array[step]
        obs["joint_action"]["vector"] = np.array(joint_vector)

        # 2. è¯»å–ç‚¹äº‘
        pointcloud = self.pointcloud_array[step]
        obs["pointcloud"] = np.array(pointcloud)

        endpose = self.endpose_array[step]
        obs["endpose"] = np.array(endpose)

        # 3. è¯»å–æŒ‡ä»¤ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.instruction is not None:
            obs["instruction"] = self.instruction
        if self.instruction_sim is not None:
            obs["instruction_sim"] = self.instruction_sim
        if self.instruction_int is not None:
            obs["instruction_int"] = self.instruction_int
            
        # ç¼“å­˜è§‚æµ‹
        self._current_obs = obs
        
        return obs
    
    def take_action_dataset(self):
        """
        è·å–å½“å‰æ­¥çš„åŠ¨ä½œæ•°æ®ï¼ˆä»å†…å­˜ä¸­è¯»å–ï¼‰
        
        Returns:
            action: å½“å‰æ­¥çš„åŠ¨ä½œæ•°ç»„
        """
        step = min(self.take_action_cnt, self.data_length - 1)
        
        # if self.verbose:
        cprint(f"ğŸ¤– è·å–åŠ¨ä½œ: step {step}/{self.data_length}", "cyan")
        
        action = self.joint_action_array[step]
        
        # return action
        self.take_action_cnt += 1
        cprint(f"â³ æ­¥æ•°: {self.take_action_cnt}/{self.step_lim}", "cyan", end="\r")
        self.robot.command_joint_state(action, "state")


    
    def get_pcd(self, color_image, depth_image, intrinsics, device='cuda'):
        """GPUåŠ é€Ÿç‰ˆï¼Œä»RGB-Dç”Ÿæˆç‚¹äº‘"""
        # è½¬tensor
        color = torch.from_numpy(color_image.copy()).float().to(device)
        depth = torch.from_numpy(depth_image.copy()).float().to(device) * 0.001

        H, W = depth.shape
        v, u = torch.meshgrid(
            torch.arange(H, device=device),
            torch.arange(W, device=device),
            indexing='ij'
        )

        valid = depth > 1e-5
        z = depth[valid]
        x = (u[valid] - intrinsics.ppx) * z / intrinsics.fx
        y = (v[valid] - intrinsics.ppy) * z / intrinsics.fy

        points = torch.stack((x, y, z), dim=-1)
        colors = color[valid]

        points = points @ self.R

        # ç­›é€‰ï¼ˆåŒæ ·åœ¨GPUä¸Šï¼‰
        valid = \
            (points[:, 0] < 2) & \
            (points[:, 1] > -3.65) & \
            (points[:, 2] > -5.75) & (points[:, 2] < -2) & \
            ~((points[:, 0] > 0.8) & (points[:, 1] > -3.57) & (points[:, 2] < -2.5))

        # valid = \
        #     (points[:, 0] < 2) & \
        #     (points[:, 1] > -3.65) & \
        #     (points[:, 2] > -5.75) & (points[:, 2] < -2)

        points, colors = points[valid], colors[valid]

        # print(f"ç‚¹äº‘åŸå§‹ç‚¹æ•°: {points.shape[0]}")

        if points.shape[0] > 2048:
            idx1 = torch.where(points[:, 1] > -3.6)[0]
            idx2 = torch.where((points[:, 1] <= -3.6) & (points[:, 1] > -3.65))[0]

            num1 = int(2048 * 0.75)
            num2 = 2048 - num1

            p1 = points[idx1].unsqueeze(0)
            p2 = points[idx2].unsqueeze(0)
            inds1 = pointnet2_utils.furthest_point_sample(p1, num1)
            inds2 = pointnet2_utils.furthest_point_sample(p2, num2)

            sampled_points = torch.cat([
                p1[0, inds1[0]], p2[0, inds2[0]]
            ], dim=0)
            sampled_colors = torch.cat([
                colors[idx1][inds1[0]], colors[idx2][inds2[0]]
            ], dim=0)

            idx = torch.randperm(2048, device=device)
            points = sampled_points[idx]
            colors = sampled_colors[idx]

        elif points.shape[0] < 2048:
            num_pad = 2048 - points.shape[0]
            pad_points = torch.zeros((num_pad, 3), device=device)
            pad_colors = torch.zeros((num_pad, 3), device=device)
            points = torch.cat([points, pad_points], dim=0)
            colors = torch.cat([colors, pad_colors], dim=0)
            idx = torch.randperm(2048, device=device)
            points = points[idx]
            colors = colors[idx]

        # åªåœ¨æœ€åè½¬æ¢ä¸ºnumpyè¿”å›
        return torch.cat([points, colors], dim=-1).cpu().numpy()

    
    def get_obs(self) -> Dict:
        """è·å–å½“å‰è§‚æµ‹
        
        Returns:
            ä¸ä»¿çœŸç¯å¢ƒæ ¼å¼ä¸€è‡´çš„è§‚æµ‹å­—å…¸:
            {
                'joint_action': {
                    'right_arm': list[6],  # Piper å•è‡‚ï¼Œå¡«å……ç©ºå€¼
                    'right_gripper': float,
                    'vector': list[7],  # 6 joints + 1 gripper
                },
                'pointcloud': np.ndarray(2048, 6)
            }
        """
        # æ ¹æ®policyæ„å»ºè§‚æµ‹å­—å…¸
        if self.policy == "DP3":
            # DP3 éœ€è¦ joint_action.vector å’Œ pointcloud
            # å¯¹äºylçš„æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¼ å…¥ instructionï¼Œinstruction_simï¼Œinstruction_int, ee_pos_quat
            robot_obs = self.robot.get_observations()
            joint_positions = robot_obs["joint_positions"]  # shape: (7,)
            ee_pos_quat = robot_obs["ee_pos_quat"]  # shape: (7,)

            # è·å–ç›¸æœºå›¾åƒ
            rgb, depth = self.camera.read(img_size=self.img_size)
            depth = depth.reshape(480, 640)
            pcd = self.get_pcd(rgb, depth, self.intrinsics)  # shape: (2048, 6)

            obs = {
                "joint_action": {
                    "vector": np.array(joint_positions),  # è¿”å› numpy arrayï¼Œä¸æ˜¯ listï¼
                },
                "endpose": np.array(ee_pos_quat).astype(np.float32),  # è¿”å› numpy arrayï¼Œä¸æ˜¯ listï¼

                "pointcloud": pcd,

                # instructions
                "instruction": self.instruction,
                "instruction_sim": self.instruction_sim,
                "instruction_int": self.instruction_int
            }
            # ä¿å­˜åˆ°/home/dex/haoran/LoopBreaker/data/tmp
            # import pickle as pkl
            # pkl.dump(obs, open(f"/home/dex/haoran/LoopBreaker/data/tmp/piper_real_dp3_obs_step{self.take_action_cnt}.pkl", "wb"))


        elif self.policy == "pi0":
            # pi0 éœ€è¦ joint_action.vector å’Œ head_camera.rgb
            robot_obs = self.robot.get_observations()
            joint_positions = robot_obs["joint_positions"]  # shape: (7,)

            # è·å–ç›¸æœºå›¾åƒ
            rgb, _ = self.camera.read(img_size=self.img_size)
            
            obs = {
                "joint_action": {
                    "vector": np.array(joint_positions),  # è¿”å› numpy arrayï¼Œä¸æ˜¯ listï¼
                },
                "observation": {
                    "head_camera": {
                        "rgb": rgb,
                    },
                },  
            }

        else:
            raise ValueError(f"æœªæ”¯æŒçš„ policy ç±»å‹: {self.policy}")
            
        return obs
    
    def take_action(self, action: np.ndarray) -> None:
        """æ‰§è¡ŒåŠ¨ä½œ
        
        Args:
            action: ç›®æ ‡å…³èŠ‚çŠ¶æ€ï¼Œshape: (7,) æˆ– (14,)
                   - å¦‚æœæ˜¯ (7,): [6ä¸ªå…³èŠ‚è§’åº¦ + 1ä¸ªå¤¹çˆªä½ç½®]
                   - å¦‚æœæ˜¯ (14,): [å·¦è‡‚6+å·¦å¤¹çˆª1 + å³è‡‚6+å³å¤¹çˆª1]ï¼Œåªä½¿ç”¨å‰7ä¸ª
        """
        self.take_action_cnt += 1
             
        # ç¡®ä¿æ˜¯ numpy æ•°ç»„
        action = np.array(action)

        print(f"ğŸ¤– æ‰§è¡ŒåŠ¨ä½œ: {action}")
        
        # å‘é€æŒ‡ä»¤åˆ°æœºå™¨äºº
        self.robot.command_joint_state(action, "state")
        
        cprint(f"â³ æ­¥æ•°: {self.take_action_cnt}/{self.step_lim}", "cyan", end="\r")
      
    def reset(self) -> None:
        """é‡ç½®æœºå™¨äººåˆ°åˆå§‹å§¿æ€"""
        # ç”¨ä¸€ä¸ªçº¿æ€§æ’å€¼å»æ§åˆ¶å¤¹çˆªåˆ°åˆå§‹ä½ç½®ï¼Œè€Œä¸æ˜¯ç›´æ¥è·³åˆ°åˆå§‹ä½ç½®
        is_replace = input("æ˜¯å¦æ”¾é”¤å­ï¼Ÿ(y/n)")
        if is_replace.lower() == 'y':
            cprint("æ”¾é”¤å­ä¸­...", "yellow")
            target = [ -2905, 111154, -59434,  -3563,  -1365,  24480, 0]
            # target = np.array()[0.12275021 -0.01490509 -0.23432243 -0.12770648  0.385226    0.44116578]
            # target = [19077, 113634, -38837, -12259, -53672, 20033, 0]
            for i in range(50):
                alpha = (i + 1) / 50.0
                interp_pos = (1 - alpha) * self.robot.get_joint_state() + alpha * np.array(target)
                # self.robot.command_joint_state(interp_pos, "state")
                joint_state_int = (interp_pos[:6]).astype(int)
                target_piper_angle = interp_pos[6]
                self.robot.piper.JointCtrl(*joint_state_int)
                self.robot.piper.GripperCtrl(
                    gripper_angle=int(target_piper_angle), 
                    gripper_effort=1000, 
                    gripper_code=0x01
                )
                time.sleep(0.03)

                # å¤¹çˆªå›åˆ°åˆå§‹ä½ç½®

            time.sleep(0.5)

            target = [ -2905, 111154, -59434,  -3563,  -1365,  24480, 70000]
            # target = [19077, 113634, -38837, -12259, -53672, 20033, 70000]

            for i in range(30):
                alpha = (i + 1) / 30.0
                interp_pos = (1 - alpha) * self.robot.get_joint_state() + alpha * np.array(target)
                joint_state_int = (interp_pos[:6]).astype(int)
                target_piper_angle = interp_pos[6]
                # self.robot.piper.JointCtrl(*joint_state_int)
                self.robot.piper.GripperCtrl(
                    gripper_angle=int(target_piper_angle), 
                    gripper_effort=1000, 
                    gripper_code=0x01
                )
                time.sleep(0.03)

            time.sleep(0.8)

        for i in range(100):
            alpha = (i + 1) / 100.0
            interp_pos = (1 - alpha) * self.robot.get_joint_state() + alpha * self.init_pos
            self.robot.command_joint_state(interp_pos, "state")
            time.sleep(0.03)
        # self.robot.command_joint_state(self.init_pos)
        self.take_action_cnt = 0
        self.eval_success = False
        time.sleep(0.5)
        cprint("âœ… å·²å°†piperé‡ç½®åˆ°åˆå§‹ä½ç½®", "green")

    def set_instruction(self, instruction: str, instruction_int: str = None, instruction_sim: str = None) -> None:
        """è®¾ç½®ä»»åŠ¡æŒ‡ä»¤ï¼ˆè¯­è¨€æè¿°ï¼‰"""
        self.instruction = instruction
        self.instruction_int = instruction_int
        self.instruction_sim = instruction_sim
        if instruction is not None:
            cprint(f"ğŸ“ ä»»åŠ¡æŒ‡ä»¤: {instruction}", "blue")
        if instruction_int is not None:
            cprint(f"ğŸ“ ä»»åŠ¡æŒ‡ä»¤_int: {instruction_int}", "blue")
        if instruction_sim is not None:
            cprint(f"ğŸ“ ä»»åŠ¡æŒ‡ä»¤_sim: {instruction_sim}", "blue")